
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>K-Means Clustering</title><meta name="generator" content="MATLAB 8.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2015-08-29"><meta name="DC.source" content="userGuide_02Clustering_02.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>K-Means Clustering</h1><!--introduction--><p>K-means clustering (k-means for short), also known as Forgy's algorithm, is one of the most well-known methods for data clustering. The goal of k-means is to find k points of a dataset that can best represent the dataset in a certain mathematical sense (to be detailed later). These k points are also known as cluster centers, prototypes, centroids, or codewords, and so on. After obtaining these cluster centers, we can use them for numerous tasks, including:</p><div><ul><li>Data compression: We can use these cluster centers to represent the original dataset. Since the number of centers is much less than the size of the original dataset, the goal of data compression can be achieved.</li><li>Data classification: We can use these cluster centers for data classification such that the computation load is lessened and the influence from noisy data is reduced.</li></ul></div><p>K-means is also a method of partitional clustering in which we need to specify the number of clusters before starting the clustering process. Suppose that the number of clusters is m, then we can define an objective function as the sum of square distances between a data point and its nearest cluster centers. We can follow a procedure to minimize the objective function iteratively by finding a new set of cluster centers that can lower the value of the objective function at each iteration.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">A basic example</a></li><li><a href="#2">Visualization of clustering process</a></li><li><a href="#4">Plot the result</a></li></ul></div><h2>A basic example<a name="1"></a></h2><p>The following example demonstrates the use of k-means on a two-dimensional dataset:</p><pre class="codeinput">DS=dcData(2);
centerNum=8;
center=kMeansClustering(DS.input, centerNum)
</pre><pre class="codeoutput">
center =

  Columns 1 through 7

    0.2238   -0.2393    0.6501   -0.6682    0.6299    0.3174   -0.3273
   -0.6705    0.6444    0.2163   -0.2397   -0.3208    0.6407   -0.6186

  Column 8

   -0.6270
    0.3083

</pre><h2>Visualization of clustering process<a name="2"></a></h2><p>If you would like to visualize the clustering process, try the next example:</p><pre class="codeinput">DS=dcData(2);
centerNum=8;
center=kMeansClustering(DS.input, centerNum, 1);
</pre><pre class="codeoutput">Iteration count = 1/200, distortion = 142.890487
Iteration count = 2/200, distortion = 69.109418
Iteration count = 3/200, distortion = 66.482128
Iteration count = 4/200, distortion = 66.115784
Iteration count = 5/200, distortion = 65.998732
Iteration count = 6/200, distortion = 65.923740
Iteration count = 7/200, distortion = 65.828486
Iteration count = 8/200, distortion = 65.713946
Iteration count = 9/200, distortion = 65.651974
Iteration count = 10/200, distortion = 65.568754
Iteration count = 11/200, distortion = 65.540425
Iteration count = 12/200, distortion = 65.520360
Iteration count = 13/200, distortion = 65.460998
Iteration count = 14/200, distortion = 65.415813
Iteration count = 15/200, distortion = 65.375092
Iteration count = 16/200, distortion = 65.329969
Iteration count = 17/200, distortion = 65.296254
Iteration count = 18/200, distortion = 65.268587
Iteration count = 19/200, distortion = 65.219556
Iteration count = 20/200, distortion = 65.149349
Iteration count = 21/200, distortion = 65.123585
Iteration count = 22/200, distortion = 65.119243
Iteration count = 23/200, distortion = 65.115730
Iteration count = 24/200, distortion = 65.113225
Iteration count = 25/200, distortion = 65.113225
</pre><img vspace="5" hspace="5" src="userGuide_02Clustering_02_01.png" alt=""> <p>MATLAB will display the animation as the clustering goes.</p><h2>Plot the result<a name="4"></a></h2><p>We can use vqDataPlot to present the result after clustering, as follows.</p><pre class="codeinput"><span class="comment">% ====== Get the data set</span>
DS = dcData(5);
subplot(2,2,1);
plot(DS.input(1,:), DS.input(2,:), <span class="string">'.'</span>);
axis <span class="string">tight</span>, axis <span class="string">image</span>
<span class="comment">% ====== Run kmeans</span>
centerNum=4;
[center, U, distortion, allCenters] = kMeansClustering(DS.input, centerNum);
<span class="comment">% ====== Plot the result</span>
subplot(2,2,2);
vqDataPlot(DS.input, center);
subplot(2,1,2);
plot(distortion, <span class="string">'o-'</span>);
xlabel(<span class="string">'No. of iterations'</span>); ylabel(<span class="string">'Distortion'</span>); grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="userGuide_02Clustering_02_02.png" alt=""> <p>The upper left plot in the above example shows the scatter plot of the dataset. The upper right plot shows the clustering results. The lower plot demonstrates how the objective function (distortion) decreases with each iteration.</p><p>Copyright 2011-2015 <a href="http://mirlab.org/jang">Jyh-Shing Roger Jang</a>.</p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2015a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% K-Means Clustering
% K-means clustering (k-means for short), also known as Forgy's algorithm, is one of the most well-known methods for data clustering. The goal of k-means is to find k points of a dataset that can best represent the dataset in a certain mathematical sense (to be detailed later). These k points are also known as cluster centers, prototypes, centroids, or codewords, and so on. After obtaining these cluster centers, we can use them for numerous tasks, including: 
%
% * Data compression: We can use these cluster centers to represent the original dataset. Since the number of centers is much less than the size of the original dataset, the goal of data compression can be achieved. 
% * Data classification: We can use these cluster centers for data classification such that the computation load is lessened and the influence from noisy data is reduced. 
%
% K-means is also a method of partitional clustering in which we need to specify the number of clusters before starting the clustering process. Suppose that the number of clusters is m, then we can define an objective function as the sum of square distances between a data point and its nearest cluster centers. We can follow a procedure to minimize the objective function iteratively by finding a new set of cluster centers that can lower the value of the objective function at each iteration.
%% A basic example
% The following example demonstrates the use of k-means on a two-dimensional dataset: 
DS=dcData(2);
centerNum=8;
center=kMeansClustering(DS.input, centerNum)
%% Visualization of clustering process
% If you would like to visualize the clustering process, try the next example:
DS=dcData(2);
centerNum=8;
center=kMeansClustering(DS.input, centerNum, 1);
%%
% MATLAB will display the animation as the clustering goes.
%% Plot the result
% We can use vqDataPlot to present the result after clustering, as follows.

% ====== Get the data set
DS = dcData(5);
subplot(2,2,1);
plot(DS.input(1,:), DS.input(2,:), '.');
axis tight, axis image
% ====== Run kmeans
centerNum=4;
[center, U, distortion, allCenters] = kMeansClustering(DS.input, centerNum);
% ====== Plot the result
subplot(2,2,2);
vqDataPlot(DS.input, center);
subplot(2,1,2);
plot(distortion, 'o-');
xlabel('No. of iterations'); ylabel('Distortion'); grid on
%%
% The upper left plot in the above example shows the scatter plot of the dataset. The upper right plot shows the clustering results. The lower plot demonstrates how the objective function (distortion) decreases with each iteration.
%%
% Copyright 2011-2015 <http://mirlab.org/jang Jyh-Shing Roger Jang>.

##### SOURCE END #####
--></body></html>